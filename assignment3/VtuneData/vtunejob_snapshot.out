vtune: Peak bandwidth measurement started.
vtune: Peak bandwidth measurement finished.
vtune: Collection started. To stop the collection, either press CTRL-C or enter from another console window: vtune -r /dss/dsshome1/05/h039var/Efficient_lab/assignment3/VtuneData/PerformanceSnapshot -command stop.
Resolutions       : (3200, 4200, ... 3200)
Iterations        : 200
Algorithm         : 0 (Jacobi)
Num. Heat sources : 2
   1: (0.00, 0.00) 1.00 1.00 
   2: (1.00, 1.00) 1.00 0.50 
Resolution:  3200residual 0.131290, 100 iterations
Resolution:  3200, Time: 4.376 (22.528 GFlop => 5148.54 MFlop/s, residual 0.046719, 200 iterations)
 3200; 4.376; 5148.537
vtune: Collection stopped.
vtune: Using result path `/dss/dsshome1/05/h039var/Efficient_lab/assignment3/VtuneData/PerformanceSnapshot'
vtune: Executing actions  0 %                                                  vtune: Executing actions  0 % Finalizing results                               vtune: Executing actions  0 % Finalizing the result                            vtune: Executing actions  0 % Clearing the database                            vtune: Executing actions  7 % Clearing the database                            vtune: Executing actions  7 % Loading raw data to the database                 vtune: Executing actions  7 % Loading 'systemcollector-1836415-i01r01c03s04.sc'vtune: Executing actions 12 % Loading 'systemcollector-1836415-i01r01c03s04.sc'vtune: Executing actions 12 % Loading '1836425.stat.perf' file                 vtune: Executing actions 12 % Updating precomputed scalar metrics              vtune: Executing actions 14 % Updating precomputed scalar metrics              vtune: Executing actions 14 % Processing profile metrics and debug information vtune: Executing actions 19 % Processing profile metrics and debug information vtune: Executing actions 19 % Setting data model parameters                    vtune: Executing actions 19 % Resolving module symbols                         vtune: Executing actions 19 % Resolving thread name information                vtune: Executing actions 21 % Resolving thread name information                vtune: Executing actions 21 % Resolving call target names for dynamic code     vtune: Executing actions 24 % Resolving call target names for dynamic code     vtune: Executing actions 24 % Resolving interrupt name information             vtune: Executing actions 26 % Resolving interrupt name information             vtune: Executing actions 26 % Processing profile metrics and debug information vtune: Executing actions 28 % Processing profile metrics and debug information vtune: Executing actions 29 % Processing profile metrics and debug information vtune: Executing actions 30 % Processing profile metrics and debug information vtune: Executing actions 31 % Processing profile metrics and debug information vtune: Executing actions 31 % Preparing output tree                            vtune: Executing actions 31 % Parsing columns in input tree                    vtune: Executing actions 32 % Parsing columns in input tree                    vtune: Executing actions 32 % Creating top-level columns                       vtune: Executing actions 32 % Creating top-level rows                          vtune: Executing actions 33 % Creating top-level rows                          vtune: Executing actions 33 % Preparing output tree                            vtune: Executing actions 33 % Parsing columns in input tree                    vtune: Executing actions 33 % Creating top-level columns                       vtune: Executing actions 34 % Creating top-level columns                       vtune: Executing actions 34 % Creating top-level rows                          vtune: Executing actions 35 % Creating top-level rows                          vtune: Executing actions 35 % Setting data model parameters                    vtune: Executing actions 35 % Precomputing frequently used data                vtune: Executing actions 35 % Precomputing frequently used data                vtune: Executing actions 36 % Precomputing frequently used data                vtune: Executing actions 37 % Precomputing frequently used data                vtune: Executing actions 38 % Precomputing frequently used data                vtune: Executing actions 39 % Precomputing frequently used data                vtune: Executing actions 40 % Precomputing frequently used data                vtune: Executing actions 41 % Precomputing frequently used data                vtune: Executing actions 41 % Updating precomputed scalar metrics              vtune: Executing actions 42 % Updating precomputed scalar metrics              vtune: Executing actions 42 % Discarding redundant overtime data               vtune: Executing actions 44 % Discarding redundant overtime data               vtune: Executing actions 44 % Saving the result                                vtune: Executing actions 46 % Saving the result                                vtune: Executing actions 48 % Saving the result                                vtune: Executing actions 49 % Saving the result                                vtune: Executing actions 50 % Saving the result                                vtune: Executing actions 50 % Generating a report                              vtune: Executing actions 50 % Setting data model parameters                    vtune: Executing actions 75 % Setting data model parameters                    vtune: Executing actions 75 % Generating a report                              Elapsed Time: 4.642s
    IPC: 1.333
    SP GFLOPS: 0.000
    DP GFLOPS: 4.780
    x87 GFLOPS: 0.000
    Average CPU Frequency: 2.294 GHz 
Logical Core Utilization: 1.0% (0.996 out of 96)
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization. Consider improving physical core utilization as the first step
 | and then look at opportunities to utilize logical cores, which in some cases
 | can improve processor throughput and overall performance of multi-threaded
 | applications.
 |
    Physical Core Utilization: 2.1% (0.996 out of 48)
     | The metric value is low, which may signal a poor physical CPU cores
     | utilization caused by:
     |     - load imbalance
     |     - threading runtime overhead
     |     - contended synchronization
     |     - thread/process underutilization
     |     - incorrect affinity that utilizes logical cores instead of physical
     |       cores
     | Run the HPC Performance Characterization analysis to estimate the
     | efficiency of MPI and OpenMP parallelism or run the Locks and Waits
     | analysis to identify parallel bottlenecks for other parallel runtimes.
     |
Microarchitecture Usage: 40.8% of Pipeline Slots
 | You code efficiency on this platform is too low.
 | 
 | Possible cause: memory stalls, instruction starvation, branch misprediction
 | or long latency instructions.
 | 
 | Next steps: Run Microarchitecture Exploration analysis to identify the cause
 | of the low microarchitecture usage efficiency.
 |
    Retiring: 40.8% of Pipeline Slots
    Front-End Bound: 5.4% of Pipeline Slots
    Bad Speculation: 1.2% of Pipeline Slots
    Back-End Bound: 52.6% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 28.9% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 4.7% of Clockticks
                FB Full: 100.0% of Clockticks
            L2 Bound: 7.7% of Clockticks
             | This metric shows how often machine was stalled on L2 cache.
             | Avoiding cache misses (L1 misses/L2 hits) will improve the
             | latency and increase performance.
             |
            L3 Bound: 0.3% of Clockticks
                L3 Latency: 1.9% of Clockticks
            DRAM Bound: 17.2% of Clockticks
             | This metric shows how often CPU was stalled on the main memory
             | (DRAM). Caching typically improves the latency and increases
             | performance.
             |
                Memory Bandwidth: 58.4% of Clockticks
                 | Issue: A significant fraction of cycles was stalled due to
                 | approaching bandwidth limits of the main memory (DRAM).
                 | 
                 | Tips: Improve data accesses to reduce cacheline transfers
                 | from/to memory using these possible techniques:
                 |     - Consume all bytes of each cacheline before it is
                 |       evicted (for example, reorder structure elements and
                 |       split non-hot ones).
                 |     - Merge compute-limited and bandwidth-limited loops.
                 |     - Use NUMA optimizations on a multi-socket system.
                 | 
                 | Note: software prefetches do not help a bandwidth-limited
                 | application.
                 |
                Memory Latency: 15.9% of Clockticks
                 | Issue: A significant fraction of cycles was stalled due to
                 | the latency of the main memory (DRAM).
                 | 
                 | Tips: Improve data accesses or interleave them with compute
                 | using such possible techniques as data layout re-structuring
                 | or software prefetches (through the compiler).
                 |
                    Local DRAM: 100.0% of Clockticks
                     | The number of CPU stalls on loads from the local memory
                     | exceeds the threshold. Consider caching data to improve
                     | the latency and increase the performance.
                     |
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 0.5% of Clockticks
        Core Bound: 23.7% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
Memory Bound: 28.9% of Pipeline Slots
 | The metric value is high. This can indicate that the significant fraction of
 | execution pipeline slots could be stalled due to demand memory load and
 | stores. Use Memory Access analysis to have the metric breakdown by memory
 | hierarchy, memory bandwidth information, correlation by memory objects.
 |
    Cache Bound: 12.7% of Clockticks
    DRAM Bound: 17.2% of Clockticks
     | The metric value is high. This indicates that a significant fraction of
     | cycles could be stalled on the main memory (DRAM) because of demand loads
     | or stores.
     |
     | The code is memory bandwidth bound, which means that there are a
     | significant fraction of cycles during which the bandwidth limits of the
     | main memory are being reached and the code could stall. Review the
     | Bandwidth Utilization Histogram to estimate the scale of the issue.
     | Improve data accesses to reduce cacheline transfers from/to memory using
     | these possible techniques: 1) consume all bytes of each cacheline before
     | it is evicted (for example, reorder structure elements and split non-hot
     | ones); 2) merge compute-limited and bandwidth-limited loops; 3) use NUMA
     | optimizations on a multi-socket system.
     |
     | The code is latency bound, which means that there are a significant
     | fraction of cycles during which the code could be stalled due to main
     | memory latency. Consider optimizing data layout or using software
     | prefetches through the compiler to improve cache reuse and to reduce the
     | data fetched from the main memory.
     |
        Average DRAM Bandwidth, GB/s: 0.000
    NUMA: % of Remote Accesses: 0.0%
Vectorization: 99.8% of Packed FP Operations
    Instruction Mix
        SP FLOPs: 0.0% of uOps
            Packed: 0.0% from SP FP
                128-bit: 0.0% from SP FP
                256-bit: 0.0% from SP FP
                512-bit: 0.0% from SP FP
            Scalar: 100.0% from SP FP
             | This code has floating point operations and is not vectorized.
             | Consider either recompiling the code with optimization options
             | that allow vectorization or using Intel Advisor to vectorize the
             | loops.
             |
        DP FLOPs: 32.1% of uOps
            Packed: 99.8% from DP FP
                128-bit: 0.0% from DP FP
                256-bit: 99.8% from DP FP
                 | A significant fraction of floating point arithmetic vector
                 | instructions executed with a partial vector load. You can try
                 | to compile the code with the latest instruction set or use
                 | Intel Advisor for vectorization help.
                 |
                512-bit: 0.0% from DP FP
            Scalar: 0.2% from DP FP
        x87 FLOPs: 0.0% of uOps
        Non-FP: 67.9% of uOps
    FP Arith/Mem Rd Instr. Ratio: 1.092
    FP Arith/Mem Wr Instr. Ratio: 6.516
Collection and Platform Info
    Application Command Line: ./heat "test.dat" 
    Operating System: 5.3.18-150300.59.63-default NAME="SLES" VERSION="15-SP3" VERSION_ID="15.3" PRETTY_NAME="SUSE Linux Enterprise Server 15 SP3" ID="sles" ID_LIKE="suse" ANSI_COLOR="0;32" CPE_NAME="cpe:/o:suse:sles:15:sp3" DOCUMENTATION_URL="https://documentation.suse.com/"
    Computer Name: i01r01c03s04
    Result Size: 3.7 MB 
    Collection start time: 14:37:03 20/05/2023 UTC
    Collection stop time: 14:37:08 20/05/2023 UTC
    Collector Type: Driverless Perf per-process counting
    CPU
        Name: Intel(R) Xeon(R) Processor code named Skylake
        Frequency: 2.694 GHz 
        Logical CPU Count: 96
        Max DRAM Single-Package Bandwidth: 115.000 GB/s
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

Recommendations:
    Hotspots: Start with Hotspots analysis to understand the efficiency of your algorithm.
     | Use Hotspots analysis to identify the most time consuming functions.
     | Drill down to see the time spent on every line of code.
    Threading: There is poor utilization of logical CPU cores (1.0%) in your application.
     |  Use Threading to explore more opportunities to increase parallelism in
     | your application.
    Microarchitecture Exploration: There is low microarchitecture usage (40.8%) of available hardware resources.
     | Run Microarchitecture Exploration analysis to analyze CPU
     | microarchitecture bottlenecks that can affect application performance.
    Memory Access: The Memory Bound metric is high  (28.9%). A significant fraction of execution pipeline slots could be stalled due to demand memory load and stores.
     | Use Memory Access analysis to measure metrics that can identify memory
     | access issues.

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
vtune: Executing actions 100 % Generating a report                             vtune: Executing actions 100 % done                                            
